data = "data"
DIRS, SAMPLES, INDICES = glob_wildcards(data+"/{dir}/train/{sample}_{index}")
TOOLS = ["learnMSA", "famsa", "t_coffee", "muscle", "clustalo", "mafft"] #"magus"

ruleorder: learnMSA > msa

#compute one output table per tool
rule all:
    input:
        expand("{tool}.tbl", tool=TOOLS)
        
        
rule msa:
    input:  
         data+"/{dir}/train/{sample}_{index}"
    output:
        "{tool}/alignments/{dir}/{sample}_{index}"
    threads: 32
    resources:
        mem_mb = 256000,
        runtime = "3d"
    priority: 1
    log:
        "{tool}/logs/{dir}/{sample}_{index}.log"
    benchmark:
        "{tool}/benchmarks/{dir}/{sample}_{index}.txt"
    params:
        tree = "{tool}/trees/{dir}/{sample}_{index}.mbed",
        tree_dir = "{tool}/trees/{dir}/",
        magus_tmp = "./magus/tmp/{sample}_{index}",
        tool = "{tool}"
    run:
        if params.tool == "famsa":
            cmd = f"famsa -t {threads} {input} {output} > {log}"
        if params.tool == "t_coffee":
            cmd = f"""mkdir -p {params.tree_dir} ; \
            clustalo --threads={threads} -i {input} --guidetree-out {params.tree} --force -o /dev/null ; \
            export MAX_N_PID_4_TCOFFEE=10000000 ; \
            t_coffee -reg -reg_method famsa_msa -reg_tree {params.tree} -seq {input} \
            -reg_nseq 1000 -reg_thread {threads} -outfile {output} > {log}"""
        if params.tool == "kalign":
            cmd = f"kalign -n {threads} -i {input} -o {output} > {log}"
        if params.tool == "clustalo":
            cmd = f"clustalo --threads={threads} -i {input} -o {output} > {log}"
        if params.tool == "magus":
            cmd = f"magus -d {params.magus_tmp} --numprocs {threads} -i {input} -o {output} > {log}"
        if params.tool == "muscle":
            cmd = f"muscle -super5 {input} -output {output} > {log}"
        if params.tool == "mafft":
            cmd = f"mafft --thread {threads} --anysymbol --quiet --dpparttree {input} > {output}"

        p = subprocess.Popen(cmd, shell= True, stdout= subprocess.PIPE, stderr= subprocess.PIPE)
        stdout, stderr= p.communicate()

        if p.returncode != 0:
            with open(output) as fout:
                fout.write("FAILED")
            print(stderr.decode(), file=sys.stderr)
            
            
         
rule learnMSA:
    input:  
        in_0 = data+"/{dir}/train/{sample}_0",
        in_1 = data+"/{dir}/train/{sample}_1",
        in_2 = data+"/{dir}/train/{sample}_2",
        in_3 = data+"/{dir}/train/{sample}_3",
    output:
        out_0 = "learnMSA/alignments/{dir}/{sample}_0",
        out_1 = "learnMSA/alignments/{dir}/{sample}_1",
        out_2 = "learnMSA/alignments/{dir}/{sample}_2",
        out_3 = "learnMSA/alignments/{dir}/{sample}_3"
    threads: 32
    resources:
        mem_mb = 1000000, #temporary to make vision work
        nvidia_gpu = 1,
        runtime = "3d",
        load = 1
    priority: 2
    log:
        log_0 = "learnMSA/logs/{dir}/{sample}_0.log",
        log_1 = "learnMSA/logs/{dir}/{sample}_1.log",
        log_2 = "learnMSA/logs/{dir}/{sample}_2.log",
        log_3 = "learnMSA/logs/{dir}/{sample}_3.log"
    benchmark:
        "learnMSA/benchmarks/{dir}/{sample}_0.txt"
    params:
        cluster_dir="learnMSA/clustering/{dir}"
    shell:
        "python3 ../../learnMSA/learnMSA.py -i {input.in_0} -o {output.out_0} -n 10 -d 0 --sequence_weights --cluster_dir {params.cluster_dir} > {log.log_0} & "
        "python3 ../../learnMSA/learnMSA.py -i {input.in_1} -o {output.out_1} -n 10 -d 1 --sequence_weights --cluster_dir {params.cluster_dir} > {log.log_1} & "
        "python3 ../../learnMSA/learnMSA.py -i {input.in_2} -o {output.out_2} -n 10 -d 2 --sequence_weights --cluster_dir {params.cluster_dir} > {log.log_2} & "
        "python3 ../../learnMSA/learnMSA.py -i {input.in_3} -o {output.out_3} -n 10 -d 3 --sequence_weights --cluster_dir {params.cluster_dir} > {log.log_3} & "
        "wait"
        
        
        
## Derive the sub-MSA of the reference sequences from the full MSA
rule project_references:
    input:
        msa = "{tool}/alignments/{dir}/{sample}_{index}",
        ref = data+"/{dir}/refs/{sample}_{index}"
    output:
        "{tool}/projections/{dir}/{sample}_{index}"
    threads: 32
    resources:
        #some aligners might produce extremely large fasta files
        #this allocates a huge chunk of memory, but the jobs are short
        mem_mb = 500000
    run:
        exit_status = open(input.msa).readline()[0].strip()
        if exit_status == "FAILED":
            # pass failed samples through
             with open(output) as fout:
                fout.write("FAILED")
        else:
            shell("""id_list=$(sed -n '/^>/p' {input.ref} | sed 's/^.//') ; \
                    export MAX_N_PID_4_TCOFFEE=10000000 ; \
                    t_coffee -other_pg seq_reformat -in {input.msa} -action +extract_seq_list ${{id_list[@]}} +rm_gap \
                    > {output}""")
        
       
## Compute a number of some general statistics and scoring metrics and store them in per-sample files
rule score_msa:
    input:
        msa = "{tool}/projections/{dir}/{sample}_{index}",
        ref = data+"/{dir}/refs/{sample}_{index}",
        all = data+"/{dir}/train/{sample}_{index}"
    output:
        "{tool}/scores/{dir}/{sample}_{index}"
    threads: 8
    resources:
        mem_mb = 16000
    run:
        exit_status = open(input.msa).readline()[0].strip()
        if exit_status == "FAILED":
            # insert a score of 0 for failed files
            shell("""nseq=$(grep -c '>' {input.all}) ; \
                    avg_len=$(awk '{{/>/&&++a||b+=length()}}END{{print b/a}}' {input.all}) ;\
                    avg_ref_len=$(awk '{{/>/&&++a||b+=length()}}END{{print b/a}}' {input.ref}) ;\
                    echo {wildcards.dir} {wildcards.sample}_{wildcards.index} $nseq $nseq_ref $avg_len $avg_ref_len $sim 0 0 0 0 >> {output}""")
        else:
            shell("""nseq=$(grep -c '>' {input.all}) ; \
                    avg_len=$(awk '{{/>/&&++a||b+=length()}}END{{print b/a}}' {input.all}) ;\
                    avg_ref_len=$(awk '{{/>/&&++a||b+=length()}}END{{print b/a}}' {input.ref}) ;\
                    export MAX_N_PID_4_TCOFFEE=10000000 ; \
                    sp_out=$(t_coffee -other_pg aln_compare -al1 {input.ref} -al2 {input.msa} -compare_mode sp | \
                    grep -v 'seq1' | grep -v '*') ; \
                    nseq_ref=$(echo $sp_out | awk '{{ print $2}}') ; \
                    sim=$(echo $sp_out | awk '{{ print $3}}') ; \
                    sp=$(echo $sp_out | awk '{{ print $4}}') ; \
                    modeler=$(t_coffee -other_pg aln_compare -al1 {input.msa}  -al2 {input.ref} -compare_mode sp | \
                    grep -v 'seq1' | grep -v '*' | awk '{{ print $4}}') ; \
                    tc=$(t_coffee -other_pg aln_compare -al1 {input.ref} -al2 {input.msa} -compare_mode tc | \
                    grep -v 'seq1' | grep -v '*' | awk '{{ print $4}}') ; \
                    col=$(t_coffee -other_pg aln_compare -al1 {input.ref} -al2 {input.msa} -compare_mode column | \
                    grep -v 'seq1' | grep -v '*' | awk '{{ print $4}}') ; \
                    echo {wildcards.dir} {wildcards.sample}_{wildcards.index} $nseq $nseq_ref $avg_len $avg_ref_len $sim $sp $modeler $tc $col >> {output}""")
        
        
rule concat_scores:
    input:
        scores = expand("{{tool}}/scores/{dir}/{sample}_{index}", zip, dir=DIRS, sample=SAMPLES, index=INDICES),
        benchmarks = expand("{{tool}}/benchmarks/{dir}/{sample}_0.txt", zip, dir=DIRS, sample=SAMPLES)
    output:
        "{tool}.tbl"
    threads: 1
    resources:
        mem_mb = 1000
    run:
        shell("echo -n "" > {output}")
        for s,b in zip(input.scores, input.benchmarks):
            #row by row for each sample, concatenate the scores and the second line of the benchmark file
            shell("echo $(cat {s}) $(tail -n 1 {b}) >> {output}")
